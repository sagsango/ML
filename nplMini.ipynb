{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nplMini.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNsJCLH4+B2cS03ZiGQdq1c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagsango/ML/blob/main/nplMini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yyiunMGJQAz"
      },
      "source": [
        "# Lesson 02: Cleaning Text Data\n",
        "import nltk\n",
        "# load data : https://www.gutenberg.org/files/120/ \n",
        "filename = '../input/ascii-test/asciiText.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "#print( text )\n",
        "#split into words\n",
        "from nltk.tokenize import word_tokenize \n",
        "tokens = word_tokenize(text)\n",
        "words = text.split()\n",
        "print( type(tokens),type(words), tokens == words )\n",
        "for i in range(len(words)):\n",
        "    if( words[i] != tokens[i] ):\n",
        "        print(words[i],tokens[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKjk1kn0xcgz"
      },
      "source": [
        "# Lesson 03: Bag-of-Words Model\n",
        "# Bag-of-Words with scikit-learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
        "\"The dog.\",\n",
        "    \"The fox\"]\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)\n",
        "# encode document\n",
        "vector = vectorizer.transform([text[0]])\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAr6HOTuxq8U"
      },
      "source": [
        "# Lesson 03: Bag-of-Words Model\n",
        "# Bag-of-Words with Keras\n",
        "from keras.preprocessing.text import Tokenizer # define 5 documents\n",
        "docs = ['Well done!',\n",
        "'Good work', 'Great effort', 'nice work', 'Excellent!']\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)\n",
        "# summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)\n",
        "# integer encode documents\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
        "print(encoded_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKWWRPDbxjAp"
      },
      "source": [
        "# Lesson 04: Word Embedding Representation\n",
        "from gensim.models import Word2Vec\n",
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "['this', 'is', 'the', 'second', 'sentence'], ['yet', 'another', 'sentence'],\n",
        "['one', 'more', 'sentence'],\n",
        "['and', 'the', 'final', 'sentence']]\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "# summarize the loaded model\n",
        "print(model)\n",
        "# summarize vocabulary\n",
        "words = list(model.wv.vocab)\n",
        "print(words)\n",
        "# access vector for one word \n",
        "print(model['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4DH2yBKx1Jl"
      },
      "source": [
        "# Lesson 04: Word Embedding Representation\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "['this', 'is', 'the', 'second', 'sentence'], ['yet', 'another', 'sentence'],\n",
        "['one', 'more', 'sentence'],\n",
        "['and', 'the', 'final', 'sentence']]\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "# fit a 2D PCA model to the vectors\n",
        "X = model[model.wv.vocab]\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "# create a scatter plot of the projection\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "words = list(model.wv.vocab)\n",
        "for i, word in enumerate(words):\n",
        "  pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRVRPqDmx9f6"
      },
      "source": [
        "# Lesson 05: Learned Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "# define problem\n",
        "vocab_size = 100\n",
        "max_length = 32\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # summarize the model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1LmbQhyyBKz"
      },
      "source": [
        "# Lesson 06: Classifying Text\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "# define problem\n",
        "vocab_size = 100\n",
        "max_length = 200\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "model.add(Conv1D(32, 8, activation='relu'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KS299T9yC27"
      },
      "source": [
        "# Lesson 07: Movie Review Sentiment Analysis"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}